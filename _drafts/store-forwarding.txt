Nehalem: The processor can forward a memory write to a subsequent read from the same address under certain conditions. A failed store forwarding will delay the subsequent read by approximately 10 clock cycles.

Store forwarding works if a write to memory is followed by a read from the same address when the read has the same operand size and the operand has its natural alignment:

try 1 iteration
; Example 8.9. Successful store-to-load forwarding
.loop
mov dword ptr [esi], eax ; 
mov ebx, dword ptr [esi] ; No stall
inc eax
dec rdi
jnz .loop

; Example 6.25. Store-to-load forwarding stall (Big read after small write)
mov byte ptr [esi], al	 : small write
mov ebx, dword ptr [esi] ; big read (stall)

Pentium:The large read after a small write prevents store-to-load forwarding, and the penalty for this is approximately 7 - 8 clock cycles.

Unlike the partial register stalls, you also get a store forwarding stall when you write a bigger operand to memory and then read part of it, if the smaller part doesn't start at the same address:

; Example 6.26. Store-to-load forwarding stall
mov dword ptr [esi], eax	; big write
mov bl, byte ptr [esi] 		; same start address (no stall).
mov bh, byte ptr [esi+1] 	; not same start address (stall). 

SandyBridge: The penalty for a failed store forwarding is approximately 12 clock cycles in most cases.

IvyBridge:
LD_BLOCKS.STORE_FORWARD
LSD.UOPS
UOPS_RETIRED.CORE_STALL_CYCLES	Cycles without actually retired uops.
UOPS_RETIRED.STALL_CYCLES	Cycles without actually retired uops.

Why is the penalty?



define_bench dendibakh_StoreForwarding_success
.loop:
mov DWORD [esi + edi * 4], edi
mov eax, DWORD [esi + edi * 4] 
dec edi
jnz .loop
ret

define_bench dendibakh_StoreForwarding_big_read_after_small_write
.loop:
mov WORD [esi + edi * 4], di   ; small write
mov eax, DWORD [esi + edi * 4] ; big read (stall)
dec edi
jnz .loop
ret

Only one blocked load can execute at a time and perhaps the subsequent stores are blocked from committing to preserve memory ordering (since load-store reordering is not allowed in x86, even though store-load is).

But according to Agner's Fog [microarchitecture.pdf](http://www.agner.org/optimize/microarchitecture.pdf) on the SandyBridge family the penalty for a failed store forwarding is approximately 12 clock cycles in most cases. But we see 14 cycles difference.

Because load-store reordering is not allowed in x86 (even though store-load is) only one blocked load can execute at a time and perhaps the subsequent stores (on next iterations) are also blocked from committing to preserve memory ordering. I think that explains why why we might have additional 2 cycles penalty, although I'm not 100% sure in that.

define_bench dendibakh_StoreForwarding_different_start_addr
.loop:
mov DWORD [esi + edi * 4], edi
mov eax, DWORD [esi + edi * 4 + 1]
dec edi
jnz .loop
ret



Event 'LD_BLOCKS.STORE_FORWARD' resolved to 'ivb::LD_BLOCKS:STORE_FORWARD:k=1:u=1:e=0:i=0:c=0:t=0, short name: 'LD_BLO' with code 0x530203
Event 'UOPS_RETIRED.STALL_CYCLES' resolved to 'ivb::UOPS_RETIRED:ALL:k=1:u=1:e=0:i=1:c=1:t=0, short name: 'UOPS_R' with code 0x1d301c2
Event 'LSD.UOPS' resolved to 'ivb::LSD:UOPS:k=1:u=1:e=0:i=0:c=0:t=0, short name: 'LSD:UO' with code 0x5301a8
Pinned to CPU 0
lipfc init OK
Running benchmarks groups using timer libpfc

** Running benchmark group StoreForwarding tests from dendibakh blog **
                     Benchmark   Cycles   LD_BLO   UOPS_R   LSD:UO
                        simple     1.02     0.00     0.02     3.87
    big read after small write    15.00     1.00    14.00     3.88
       different start address    15.33     1.00    14.33     3.84



    std::shared_ptr<BenchmarkGroup> dendibakh_StoreForwarding_group = std::make_shared<BenchmarkGroup>("dendibakh_StoreForwarding", "StoreForwarding tests from dendibakh blog");

    const int StoreForwarding_iters = 1000;

	class pr
	{
	public:
		int StoreForwarding_ptr[StoreForwarding_iters + 10];
		~pr()
		{
		    /*for (int i = 0; i < StoreForwarding_iters; ++i)
		    {
			std::cout << StoreForwarding_ptr[i] << " ";
		    }*/
		}
	};

static pr p;

    for (int i = 0; i < StoreForwarding_iters + 10; ++i)
    {
	p.StoreForwarding_ptr[i] = 0;
    }

    dendibakh_StoreForwarding_group->add(std::vector<Benchmark> {
        default_maker::template make_bench<dummy_bench,dendibakh_StoreForwarding_success>  (dendibakh_StoreForwarding_group.get(),   "StoreForwarding",  "simple",  1, []{ return p.StoreForwarding_ptr; }, StoreForwarding_iters),
        default_maker::template make_bench<dummy_bench,dendibakh_StoreForwarding_big_read_after_small_write>  (dendibakh_StoreForwarding_group.get(),   "StoreForwarding",  "big read after small write",  1, []{ return p.StoreForwarding_ptr; }, StoreForwarding_iters),
        default_maker::template make_bench<dummy_bench,dendibakh_StoreForwarding_different_start_addr>  (dendibakh_StoreForwarding_group.get(),   "StoreForwarding",  "different start address",  1, []{ return p.StoreForwarding_ptr; }, StoreForwarding_iters),
    });
    list.push_back(dendibakh_StoreForwarding_group);
