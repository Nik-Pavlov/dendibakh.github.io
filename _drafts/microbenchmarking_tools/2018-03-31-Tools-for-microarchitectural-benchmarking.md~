---
layout: post
title: Overview of the tools for microarchitectural benchmarking.
tags: default
---

I did a fair amount of low level experiments during the recent months and I tried different tools for making such experiments.

*Disclaimer:*
In this post I just want to bring everything that I know in one common place. **I don't want to compare different tools.**

### What do I mean by microarchitectural benchmarking?

Modern computers are so complicated that it's really hard to measure something in isolation. It's not enough to just run your benchmark and measure execution time. You need to think about context switches, CPU frequency scaling features (called "turboboost"), etc. There are a lot of details that can affect execution time.

What would you do if you want just to benchmark two assembly sequences? Or you want to experiment with some HW feature to see how it works?

Even if my benchmark is a simple loop inside `main` and I measure execution time of the whole binary - that's not a benchmark that I want because there is a lot of code that runs before main - it will add a lot of noise. Also if I will collect performance counters with `perf stat -e` it will add a lot of noise in the results. What I want is fine-grained performance counters collection for some specified region, not the whole execution time.

In this post I will quickly give you a taste of the tools available without going to much into the details. Also we need to distinguish between static and dynamic tools. 

Static tools don't run the actual code but try to simulate the execution keeping as much microarchitectural details as they can. Of course they are not capable of doing real measurements (execution time, performance counters) because they don't run the code. The good thing about that is that you don't need to have the real HW. You don't need to have privileged access rights as well. Another benefit is that you don't need to worry about consistency of the results. They will be always consistent because execution is not biased in any way. Today we will look into two examples of such tools: [IACA]() and [llvm-mca]().

Dynamic tools are based on running the code on the real HW and collecting all sorts of information about the execution. The good thing about that is that this is the only 100% reliable method of proving things. Usually static tools still can't predict and simulate everything inside modern CPUs.


